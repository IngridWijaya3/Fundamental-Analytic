{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> SENTIMENT CLASSIFCATION USING VADER, NAIVE BAYES & LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymongo\n",
    "import json\n",
    "\n",
    "# Connect to Mongo and Get Databases\n",
    "\n",
    "mongo_connect_string = 'mongodb://TwitterIPO?authSource=TwitterIPO'\n",
    "\n",
    "client = pymongo.MongoClient(mongo_connect_string)\n",
    "db = client.TwitterIPO\n",
    "RawTweets = db.RawTweets\n",
    "ProcessedTweets = db.ProcessedTweets\n",
    "PriceData = db.PriceData\n",
    "Vader = db.Vader\n",
    "MNB_Logit_SVM_Sentiment = db.MNB_Logit_SVM_Sentiment2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_tweet(t):\n",
    "\n",
    "    t = t.replace('RT ','')\n",
    "    t = t.replace('$','')\n",
    "    t = t.replace('#','')\n",
    "    \n",
    "    for word in t:\n",
    "        if word.startswith((\"@\")) == True:\n",
    "            t = t.replace(word,'')\n",
    "            \n",
    "    t = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''','',t)\n",
    "    t = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", t)\n",
    "    t = t.replace(',','')\n",
    "    t = t.replace('.','')\n",
    "    t = t.replace(';','')\n",
    "    t = t.replace(\"'\",'')\n",
    "    t = t.replace('\"','')\n",
    "    t = t.replace('...','')\n",
    "    t = t.lower()\n",
    "    \n",
    "    return(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all records from RawTweets DB\n",
    "tweetdb = list(ProcessedTweets.find())\n",
    "\n",
    "# Clean tweets up. Remove URLs, RTs, @..'s, etc\n",
    "result = []\n",
    "\n",
    "for tweet in tweetdb:\n",
    "    #print(tweet['text'])\n",
    "    tweet['text'] = clean_tweet(tweet['text'])\n",
    "    result.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>_id</th>\n",
       "      <th>company</th>\n",
       "      <th>datems</th>\n",
       "      <th>datepy</th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>743562292776730624_ACIA</td>\n",
       "      <td>58f187361845122414a0b5ce</td>\n",
       "      <td>ACIA</td>\n",
       "      <td>1466114039000</td>\n",
       "      <td>2016-06-16 13:53:59</td>\n",
       "      <td>en</td>\n",
       "      <td>acia a picture is worth a thousand words marke...</td>\n",
       "      <td>743562292776730624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>743488689129594881_ACIA</td>\n",
       "      <td>58f187361845122414a0b5cf</td>\n",
       "      <td>ACIA</td>\n",
       "      <td>1466096491000</td>\n",
       "      <td>2016-06-16 09:01:31</td>\n",
       "      <td>en</td>\n",
       "      <td>acia a very powerful hightight flag set up tha...</td>\n",
       "      <td>743488689129594881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>743489909328089088_ACIA</td>\n",
       "      <td>58f187361845122414a0b5d0</td>\n",
       "      <td>ACIA</td>\n",
       "      <td>1466096782000</td>\n",
       "      <td>2016-06-16 09:06:22</td>\n",
       "      <td>en</td>\n",
       "      <td>perfect on acia ross! 3932 4310\\nif you need a...</td>\n",
       "      <td>743489909328089088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>743590750559027201_ACIA</td>\n",
       "      <td>58f187361845122414a0b5d1</td>\n",
       "      <td>ACIA</td>\n",
       "      <td>1466120824000</td>\n",
       "      <td>2016-06-16 15:47:04</td>\n",
       "      <td>en</td>\n",
       "      <td>acia scwx mrk pen z zg mdvn veev some idea we ...</td>\n",
       "      <td>743590750559027201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>743485193542049794_ACIA</td>\n",
       "      <td>58f187361845122414a0b5d2</td>\n",
       "      <td>ACIA</td>\n",
       "      <td>1466095658000</td>\n",
       "      <td>2016-06-16 08:47:38</td>\n",
       "      <td>en</td>\n",
       "      <td>acia unusually strong name</td>\n",
       "      <td>743485193542049794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       UID                       _id company         datems  \\\n",
       "0  743562292776730624_ACIA  58f187361845122414a0b5ce    ACIA  1466114039000   \n",
       "1  743488689129594881_ACIA  58f187361845122414a0b5cf    ACIA  1466096491000   \n",
       "2  743489909328089088_ACIA  58f187361845122414a0b5d0    ACIA  1466096782000   \n",
       "3  743590750559027201_ACIA  58f187361845122414a0b5d1    ACIA  1466120824000   \n",
       "4  743485193542049794_ACIA  58f187361845122414a0b5d2    ACIA  1466095658000   \n",
       "\n",
       "               datepy lang                                               text  \\\n",
       "0 2016-06-16 13:53:59   en  acia a picture is worth a thousand words marke...   \n",
       "1 2016-06-16 09:01:31   en  acia a very powerful hightight flag set up tha...   \n",
       "2 2016-06-16 09:06:22   en  perfect on acia ross! 3932 4310\\nif you need a...   \n",
       "3 2016-06-16 15:47:04   en  acia scwx mrk pen z zg mdvn veev some idea we ...   \n",
       "4 2016-06-16 08:47:38   en                         acia unusually strong name   \n",
       "\n",
       "             tweet_id  \n",
       "0  743562292776730624  \n",
       "1  743488689129594881  \n",
       "2  743489909328089088  \n",
       "3  743590750559027201  \n",
       "4  743485193542049794  "
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DF with IPO Tweets\n",
    "\n",
    "df_IPOTWEETS = pd.DataFrame(result)\n",
    "df_IPOTWEETS = df_IPOTWEETS[df_IPOTWEETS['lang'] == 'en']\n",
    "df_IPOTWEETS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Incorporate Kaggle/UMich/Senti140 Dataset (1.5m tagged tweets - not all by humans though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1578613, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saifi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "## Use this to incorporate the 1.5m tweet dataset\n",
    "## Transform Dataset\n",
    "\n",
    "\n",
    "#df_senti15 = pd.read_csv('Senti_Kaggle_UMICH.csv', error_bad_lines=False)\n",
    "#df_senti15 = df_senti15[['Sentiment','SentimentText']]\n",
    "#df_senti15 = df_senti15.rename(columns={'Sentiment' : 'label_num','SentimentText' : 'text'})\n",
    "\n",
    "df_senti15 = df_senti15[['text','label_num']]\n",
    "df_senti15['label'] = df_senti15['label_num'].map({0 : 'Negative', 1 : 'Positive'})\n",
    "\n",
    "print(df_senti15.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Import NLTK Corpus/Brown/ and Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve NLTK Twitter Data and save to CSVs\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "path_list = [twitter_samples.abspath(i) for i in twitter_samples.fileids()]\n",
    "\n",
    "from nltk.twitter.common import json2csv\n",
    "\n",
    "for file in path_list:\n",
    "    with open(file) as fp:\n",
    "        json2csv(fp, os.getcwd() + \"\\\\\" + str(file)[-20:-5] +'.csv',['text'])\n",
    "        \n",
    "# Read positive and negative tweets from NLTK CSVs\n",
    "\n",
    "df_neg = pd.DataFrame.from_csv('negative_tweets.csv' , index_col=None)\n",
    "df_pos = pd.DataFrame.from_csv('positive_tweets.csv', index_col=None)\n",
    "\n",
    "df_test = pd.DataFrame.from_csv('20150430-223406.csv', index_col=None)\n",
    "\n",
    "\n",
    "df_neg['label'] = 'Negative'\n",
    "df_pos['label'] = 'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Join and assign labels to the dataset\n",
    "df_train = pd.concat([df_neg, df_pos,df_senti15], ignore_index=True)\n",
    "df_train['label_num'] = df_train.label.map({'Negative':0, 'Positive':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read Sander's Corpus\n",
    "df_Brown = pd.read_csv('sanders_corpus.csv')\n",
    "df_Brown.head()\n",
    "\n",
    "# Assign column labels\n",
    "df_Brown = df_Brown[['TweetText','Sentiment']]\n",
    "df_Brown.columns = ['text','label']\n",
    "\n",
    "# Remove Irrelevant and Neutral\n",
    "df_Brown = df_Brown[df_Brown['label'] != 'irrelevant']\n",
    "df_Brown = df_Brown[df_Brown['label'] != 'neutral']\n",
    "df_Brown['label_num'] = df_Brown.label.map({'negative' : 0, 'positive' : 1})\n",
    "\n",
    "# Join Brown Corpus to NLTK Twitter Data\n",
    "df_train = pd.concat([df_train, df_Brown],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1589704, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                  hopeless for tmr :(\n",
       "1    everything in the kids section of ikea is so c...\n",
       "2    hegelbon that heart sliding into the waste bas...\n",
       "3    “ketchburning: i hate japanese call him bani :...\n",
       "4               dang starting next week i have work :(\n",
       "5                           oh god my babies faces :( \n",
       "6                     rileymcdonough make me smile :((\n",
       "7    f0ggstar stuartthull work neighbour on motors ...\n",
       "8                            why?:(tahuodyy: sialan:( \n",
       "9    athabasca glacier was there in :-( athabasca g...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Clean Tweets\n",
    "df_train['text'] = df_train['text'].map(lambda x: clean_tweet(x))\n",
    "print(df_train.shape)\n",
    "df_train['text'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Accuracy for 75/25 Split of NLTK Twitter Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1192278,)\n",
      "(397426,)\n",
      "(1192278,)\n",
      "(397426,)\n",
      "Wall time: 1.52 s\n",
      "Naive Bayes Accuracy 0.804099379507\n",
      "Wall time: 16min 43s\n",
      "LOGISTIC Regression Accuracy 0.825514687011\n",
      "SVM Accuracy 0.827391766014\n"
     ]
    }
   ],
   "source": [
    "# Assign X and Y\n",
    "X = df_train['text']\n",
    "y = df_train['label_num']\n",
    "\n",
    "# split X and y into training and testing sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# import and instantiate CountVectorizer (with the default parameters)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# instantiate the vectorizer\n",
    "vect = CountVectorizer(ngram_range=(1, 3))\n",
    "#vect = CountVectorizer()\n",
    "\n",
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "# examine the document-term matrix\n",
    "X_train_dtm\n",
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm\n",
    "\n",
    "# import and instantiate a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# train the model using X_train_dtm (timing it with an IPython \"magic command\")\n",
    "%time nb.fit(X_train_dtm, y_train)\n",
    "\n",
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "print(\"Naive Bayes Accuracy\", metrics.accuracy_score(y_test, y_pred_class))\n",
    "\n",
    "# print the confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "# Check False Pos and False Neg (change sign < or >)\n",
    "#X_test[(y_pred_class > y_test)]\n",
    "\n",
    "# calculate predicted probabilities for X_test_dtm (poorly calibrated)\n",
    "#y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\n",
    "#y_pred_prob\n",
    "\n",
    "# calculate AUC\n",
    "#metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "############# LOGISTIC Regression ###############\n",
    "\n",
    "# import and instantiate a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# train the model using X_train_dtm\n",
    "%time logreg.fit(X_train_dtm, y_train)\n",
    "\n",
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "\n",
    "# calculate accuracy\n",
    "print(\"LOGISTIC Regression Accuracy\", metrics.accuracy_score(y_test, y_pred_class))\n",
    "\n",
    "# print the confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "# print message text for the false positives (ham incorrectly classified as spam)\n",
    "#X_test[(y_pred_class==1) & (y_test==0)]\n",
    "\n",
    "# calculate predicted probabilities for X_test_dtm (well calibrated)\n",
    "#y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
    "#y_pred_prob\n",
    "\n",
    "# calculate AUC\n",
    "#metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "########## SVM ##############\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfid_vect = TfidfVectorizer(sublinear_tf=True,\n",
    "                             use_idf=True,\n",
    "                           ngram_range=(1,3))\n",
    "train_vectors = tfid_vect.fit_transform(X_train)\n",
    "test_vectors = tfid_vect.transform(X_test)\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "# Train the classifier\n",
    "classifier_rbf = svm.LinearSVC()\n",
    "classifier_rbf.fit(train_vectors, y_train)\n",
    "\n",
    "# Predict the classifier\n",
    "prediction_rbf = classifier_rbf.predict(test_vectors)\n",
    "\n",
    "# Add to dataframe\n",
    "print(\"SVM Accuracy\", metrics.accuracy_score(y_test, prediction_rbf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> BUILD DOCUMENT MATRIX FOR IPO TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1589704,)\n",
      "(1589704,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1589704x15212304 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 52758599 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define X and y (from the Twitter data) for use with COUNTVECTORIZER\n",
    "X_train = df_train.text\n",
    "y_train = df_train.label_num\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# import and instantiate CountVectorizer (with the default parameters)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# instantiate the vectorizer\n",
    "vect = CountVectorizer(ngram_range=(1, 3))\n",
    "#vect = CountVectorizer()\n",
    "\n",
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "# examine the document-term matrix\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<23749x15212304 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 418465 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dtm = vect.transform(df_IPOTWEETS.text)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.34 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Naive Bayes Prediction\n",
    "%time nb.fit(X_train_dtm, y_train)\n",
    "\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "df_IPOTWEETS['mnb_predict'] = y_pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> LOGIT Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26min 35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import and instantiate a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# train the model using X_train_dtm\n",
    "%time logreg.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "df_IPOTWEETS['logit_predict'] = y_pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> LinearSVM </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfid_vect = TfidfVectorizer(sublinear_tf=True,\n",
    "                             use_idf=True,\n",
    "                           ngram_range=(1,3))\n",
    "train_vectors = tfid_vect.fit_transform(X_train)\n",
    "test_vectors = tfid_vect.transform(df_IPOTWEETS['text'])\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "# Train the classifier\n",
    "classifier_rbf = svm.LinearSVC()\n",
    "classifier_rbf.fit(train_vectors, y_train)\n",
    "\n",
    "# Predict the classifier\n",
    "prediction_rbf = classifier_rbf.predict(test_vectors)\n",
    "\n",
    "# Add to dataframe\n",
    "df_IPOTWEETS['svm_predict'] = prediction_rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Add to MongoDB\n",
    "\n",
    "for index,row in df_IPOTWEETS.iterrows():\n",
    "    senti_dict = {}\n",
    "    senti_dict['UID'] = row['UID']\n",
    "    senti_dict['mnb_predict'] = row['mnb_predict']\n",
    "    senti_dict['logit_predict'] = row['logit_predict']\n",
    "    senti_dict['svm_predict'] = row['svm_predict']\n",
    "    MNB_Logit_SVM_Sentiment.insert_one(senti_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> VADER Scores </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Sentiment Analysis using Vader\n",
    "#### Test Version\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sentiment = []\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for index,row in df_IPOTWEETS.iterrows():\n",
    "    #print(sentence)\n",
    "    ss = sid.polarity_scores(row['text'])\n",
    "    ss['UID'] = row['UID']\n",
    "    Vader.insert_one(ss)\n",
    "    \n",
    "# Check Vader Collection\n",
    "print(Vader.count())\n",
    "#print(Vader.find_one())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
