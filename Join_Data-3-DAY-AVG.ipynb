{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymongo\n",
    "import json\n",
    "\n",
    "# Connect to Mongo and Get Databases\n",
    "\n",
    "mongo_connect_string = 'mongodb://TwitterIPO?authSource=TwitterIPO'\n",
    "\n",
    "client = pymongo.MongoClient(mongo_connect_string)\n",
    "db = client.TwitterIPO\n",
    "RawTweets = db.RawTweets\n",
    "ProcessedTweets = db.ProcessedTweets\n",
    "PriceData = db.PriceData\n",
    "Vader = db.Vader\n",
    "MNB_Logit_SVM_Sentiment = db.MNB_Logit_SVM_Sentiment2\n",
    "TweetWordCount = db.TweetWordCount\n",
    "FinalDataset = db.FinalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj_Close</th>\n",
       "      <th>Adj_High</th>\n",
       "      <th>Adj_Low</th>\n",
       "      <th>Adj_Open</th>\n",
       "      <th>Adj_Volume</th>\n",
       "      <th>Close</th>\n",
       "      <th>Date</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Name</th>\n",
       "      <th>Open</th>\n",
       "      <th>Split</th>\n",
       "      <th>UID</th>\n",
       "      <th>Volume</th>\n",
       "      <th>_id</th>\n",
       "      <th>class_num_label</th>\n",
       "      <th>direction</th>\n",
       "      <th>return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.89</td>\n",
       "      <td>20.69</td>\n",
       "      <td>19.75</td>\n",
       "      <td>20.65</td>\n",
       "      <td>25630157.0</td>\n",
       "      <td>19.89</td>\n",
       "      <td>2017-03-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.69</td>\n",
       "      <td>19.75</td>\n",
       "      <td>SNAP</td>\n",
       "      <td>20.65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SNAP10</td>\n",
       "      <td>25630157.0</td>\n",
       "      <td>58efeb8b1845123420836caf</td>\n",
       "      <td>0</td>\n",
       "      <td>Down</td>\n",
       "      <td>-0.042369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.82</td>\n",
       "      <td>22.25</td>\n",
       "      <td>20.52</td>\n",
       "      <td>20.65</td>\n",
       "      <td>47599301.0</td>\n",
       "      <td>21.82</td>\n",
       "      <td>2017-03-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.25</td>\n",
       "      <td>20.52</td>\n",
       "      <td>SNAP</td>\n",
       "      <td>20.65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SNAP14</td>\n",
       "      <td>47599301.0</td>\n",
       "      <td>58efeb8b1845123420836cb0</td>\n",
       "      <td>1</td>\n",
       "      <td>Up</td>\n",
       "      <td>0.070658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.81</td>\n",
       "      <td>23.43</td>\n",
       "      <td>21.31</td>\n",
       "      <td>22.03</td>\n",
       "      <td>49834423.0</td>\n",
       "      <td>22.81</td>\n",
       "      <td>2017-03-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.43</td>\n",
       "      <td>21.31</td>\n",
       "      <td>SNAP</td>\n",
       "      <td>22.03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SNAP4</td>\n",
       "      <td>49834423.0</td>\n",
       "      <td>58efeb8b1845123420836cb1</td>\n",
       "      <td>1</td>\n",
       "      <td>Up</td>\n",
       "      <td>0.063899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.38</td>\n",
       "      <td>20.54</td>\n",
       "      <td>19.55</td>\n",
       "      <td>20.04</td>\n",
       "      <td>20080142.0</td>\n",
       "      <td>20.38</td>\n",
       "      <td>2017-03-21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.54</td>\n",
       "      <td>19.55</td>\n",
       "      <td>SNAP</td>\n",
       "      <td>20.04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SNAP13</td>\n",
       "      <td>20080142.0</td>\n",
       "      <td>58efeb8b1845123420836cb2</td>\n",
       "      <td>1</td>\n",
       "      <td>Up</td>\n",
       "      <td>0.022579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.44</td>\n",
       "      <td>22.50</td>\n",
       "      <td>20.64</td>\n",
       "      <td>22.21</td>\n",
       "      <td>71849684.0</td>\n",
       "      <td>21.44</td>\n",
       "      <td>2017-03-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.50</td>\n",
       "      <td>20.64</td>\n",
       "      <td>SNAP</td>\n",
       "      <td>22.21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SNAP3</td>\n",
       "      <td>71849684.0</td>\n",
       "      <td>58efeb8b1845123420836cb3</td>\n",
       "      <td>0</td>\n",
       "      <td>Down</td>\n",
       "      <td>-0.098023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Adj_Close  Adj_High  Adj_Low  Adj_Open  Adj_Volume  Close        Date  \\\n",
       "0      19.89     20.69    19.75     20.65  25630157.0  19.89  2017-03-16   \n",
       "1      21.82     22.25    20.52     20.65  47599301.0  21.82  2017-03-22   \n",
       "2      22.81     23.43    21.31     22.03  49834423.0  22.81  2017-03-08   \n",
       "3      20.38     20.54    19.55     20.04  20080142.0  20.38  2017-03-21   \n",
       "4      21.44     22.50    20.64     22.21  71849684.0  21.44  2017-03-07   \n",
       "\n",
       "   Dividend   High    Low  Name   Open  Split     UID      Volume  \\\n",
       "0       0.0  20.69  19.75  SNAP  20.65    1.0  SNAP10  25630157.0   \n",
       "1       0.0  22.25  20.52  SNAP  20.65    1.0  SNAP14  47599301.0   \n",
       "2       0.0  23.43  21.31  SNAP  22.03    1.0   SNAP4  49834423.0   \n",
       "3       0.0  20.54  19.55  SNAP  20.04    1.0  SNAP13  20080142.0   \n",
       "4       0.0  22.50  20.64  SNAP  22.21    1.0   SNAP3  71849684.0   \n",
       "\n",
       "                        _id  class_num_label direction    return  \n",
       "0  58efeb8b1845123420836caf                0      Down -0.042369  \n",
       "1  58efeb8b1845123420836cb0                1        Up  0.070658  \n",
       "2  58efeb8b1845123420836cb1                1        Up  0.063899  \n",
       "3  58efeb8b1845123420836cb2                1        Up  0.022579  \n",
       "4  58efeb8b1845123420836cb3                0      Down -0.098023  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Price Date and Create a Dataframe\n",
    "prices = [x for x in PriceData.find()]\n",
    "df_prices = pd.DataFrame(prices)\n",
    "df_prices['Date'] = pd.to_datetime(df_prices.Date)\n",
    "# Format the date\n",
    "df_prices['Date'] = df_prices.Date.apply(lambda x: datetime.utcfromtimestamp(x.value / 1e9).date())\n",
    "df_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23749, 8)\n"
     ]
    }
   ],
   "source": [
    "# Get Twitter Data\n",
    "df_tweets = pd.DataFrame(list(ProcessedTweets.find({'lang' : 'en'})))\n",
    "df_tweets.head()\n",
    "print(df_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get Sentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23749, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_MNBLogSVM = pd.DataFrame(list(MNB_Logit_SVM_Sentiment.find()))\n",
    "df_MNBLogSVM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge MNBLogSVM Data with Twitter Data\n",
    "df_senti = pd.merge(df_tweets, df_MNBLogSVM, on='UID',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert Merged Date to Date format for joining\n",
    "df_senti.datepy = df_senti.datepy.map(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map into Positive and Negative\n",
    "df_senti['logit_label'] = df_senti.logit_predict.map({1 : 'Positive', 0 : 'Negative'})\n",
    "df_senti['mnb_label'] = df_senti.mnb_predict.map({1 : 'Positive', 0 : 'Negative'})\n",
    "df_senti['svm_label'] = df_senti.svm_predict.map({1 : 'Positive', 0 : 'Negative'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep columns that we want. Get rid of the rest. Divide into Seperate Tables for SVM, LOGIT & MNB for easier joining.\n",
    "\n",
    "df_svm = df_senti[['company', 'datepy', 'svm_label']]\n",
    "df_mnb = df_senti[['company', 'datepy', 'mnb_label']]\n",
    "df_logit = df_senti[['company', 'datepy', 'logit_label']]\n",
    "\n",
    "# Get Counts of each\n",
    "\n",
    "svm_counts = df_svm.groupby(by=['company','datepy'])\n",
    "svm_counts = svm_counts.svm_label.value_counts()\n",
    "\n",
    "mnb_counts = df_mnb.groupby(by=['company','datepy'])\n",
    "mnb_counts = mnb_counts.mnb_label.value_counts()\n",
    "\n",
    "logit_counts = df_logit.groupby(by=['company','datepy'])\n",
    "logit_counts = logit_counts.logit_label.value_counts()\n",
    "\n",
    "# Turn into dicts for joining\n",
    "\n",
    "svm_dict = []\n",
    "for k,v in svm_counts.iteritems():\n",
    "    s = {}\n",
    "    s['Name'] = k[0]\n",
    "    s['Date'] = k[1]\n",
    "    \n",
    "    if k[2] == 'Positive':\n",
    "        s['SVM_Pos_Count'] = v\n",
    "    else:\n",
    "        s['SVM_Neg_Count'] = v\n",
    "\n",
    "    svm_dict.append(s)\n",
    "    \n",
    "mnb_dict = []\n",
    "for k,v in mnb_counts.iteritems():\n",
    "    s = {}\n",
    "    s['Name'] = k[0]\n",
    "    s['Date'] = k[1]\n",
    "    \n",
    "    if k[2] == 'Positive':\n",
    "        s['MNB_Pos_Count'] = v\n",
    "    else:\n",
    "        s['MNB_Neg_Count'] = v\n",
    "\n",
    "    mnb_dict.append(s)\n",
    "    \n",
    "logit_dict = []\n",
    "for k,v in logit_counts.iteritems():\n",
    "    \n",
    "    log_res = {}\n",
    "    \n",
    "    log_res['Name'] = k[0]\n",
    "    \n",
    "    log_res['Date'] = k[1]\n",
    "    \n",
    "    if k[2] == 'Positive':\n",
    "        log_res['Logit_Pos_Count'] = v\n",
    "    else:\n",
    "        log_res['Logit_Neg_Count'] = v\n",
    "\n",
    "    logit_dict.append(log_res)\n",
    "\n",
    "    \n",
    "# Convert back to dataframes and reset index\n",
    "\n",
    "svm_counts = pd.DataFrame(svm_dict)\n",
    "svm_counts = svm_counts.groupby(['Name','Date']).sum()\n",
    "svm_counts = svm_counts.fillna(0)\n",
    "svm_counts = svm_counts.reset_index(level=['Date','Name'])\n",
    "\n",
    "mnb_counts = pd.DataFrame(mnb_dict)\n",
    "mnb_counts = mnb_counts.groupby(['Name','Date']).sum()\n",
    "mnb_counts = mnb_counts.fillna(0)\n",
    "mnb_counts = mnb_counts.reset_index(level=['Date','Name'])\n",
    "\n",
    "logit_counts = pd.DataFrame(logit_dict)\n",
    "logit_counts = logit_counts.groupby(['Name','Date']).sum()\n",
    "logit_counts = logit_counts.fillna(0)\n",
    "logit_counts = logit_counts.reset_index(level=['Date','Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> TweetWordCount - Average Ratio & Counts </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Tweet Word Count and merge with Processed Tweets\n",
    "df_tweetwordcount = pd.DataFrame(list(TweetWordCount.find()))\n",
    "df_tweetwordcount = pd.merge(df_tweets, df_tweetwordcount, on='UID',how='inner')\n",
    "\n",
    "# Convert Datetime object to date\n",
    "df_tweetwordcount.datepy = df_tweetwordcount.datepy.map(lambda x: x.date())\n",
    "\n",
    "# Add Labels\n",
    "df_tweetwordcount['twc_label'] = np.where(df_tweetwordcount['ratio'] > 0, 'Positive','Negative')\n",
    "df_tweetwordcount.loc[df_tweetwordcount.ratio == 0, 'twc_label'] = 'Neutral'\n",
    "\n",
    "# Groupby Date and Company for Average Ratio\n",
    "df_tweetwordcount_ratio = df_tweetwordcount.groupby(['datepy','company']).mean()\n",
    "\n",
    "# Keep Date, Company & Ratio\n",
    "df_tweetwordcount_ratio = df_tweetwordcount_ratio[['ratio']]\n",
    "df_tweetwordcount_ratio = df_tweetwordcount_ratio.reset_index(['datepy','company'])\n",
    "df_tweetwordcount_ratio = df_tweetwordcount_ratio.rename(columns={'datepy' : 'Date','company':'Name','ratio' : 'WordCountRatio'})\n",
    "\n",
    "# Get Counts\n",
    "\n",
    "df_tweetwordcount_counts = df_tweetwordcount[['company', 'datepy', 'twc_label']]\n",
    "\n",
    "twc_counts = df_tweetwordcount_counts.groupby(by=['company','datepy'])\n",
    "twc_counts = twc_counts.twc_label.value_counts()\n",
    "\n",
    "# Turn counts into DF for easier conversion to pandas object\n",
    "twc_dict = []\n",
    "for k,v in twc_counts.iteritems():\n",
    "    s = {}\n",
    "    s['Name'] = k[0]\n",
    "    s['Date'] = k[1]\n",
    "    \n",
    "    if k[2] == 'Positive':\n",
    "        s['TWC_Pos_Count'] = v\n",
    "    elif k[2] == 'Neutral':\n",
    "        s['TWC_Neutral_Count'] = v\n",
    "    else:\n",
    "        s['TWC_Neg_Count'] = v\n",
    "\n",
    "    twc_dict.append(s)\n",
    "\n",
    "# Turn TWC Dict to Dataframe\n",
    "\n",
    "twc_counts = pd.DataFrame(twc_dict)\n",
    "twc_counts = twc_counts.groupby(['Name','Date']).sum()\n",
    "twc_counts = twc_counts.fillna(0)\n",
    "twc_counts = twc_counts.reset_index(level=['Date','Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Vader Incorporation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saifi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Get Vader and merge with Processed Tweets\n",
    "df_vader = pd.DataFrame(list(Vader.find()))\n",
    "df_vader = pd.merge(df_tweets, df_vader, on='UID',how='inner')\n",
    "\n",
    "# Convert Datetime object to date\n",
    "df_vader.datepy = df_vader.datepy.map(lambda x: x.date())\n",
    "\n",
    "# Add Labels\n",
    "df_vader['vader_label'] = np.where(df_vader['compound'] > 0, 'Positive','Negative')\n",
    "criterion = df_vader['compound'].map(lambda x: x == 0)\n",
    "df_vader['vader_label'][criterion] = 'Neutral'\n",
    "\n",
    "# Groupby Date and Company for Average compound\n",
    "df_vader_score = df_vader.groupby(['datepy','company']).mean()\n",
    "\n",
    "# Keep Date, Company & compound\n",
    "df_vader_score = df_vader_score[['compound','neg','neu','pos']]\n",
    "df_vader_score = df_vader_score.reset_index(['datepy','company'])\n",
    "df_vader_score = df_vader_score.rename(columns={'datepy' : 'Date',\n",
    "                                                'company':'Name',\n",
    "                                                'compound' : 'Vader_Compound',\n",
    "                                                'neg' : 'Vader_Neg',\n",
    "                                                'neu': 'Vader_Neu',\n",
    "                                                'pos' : 'Vader_Pos'})\n",
    "\n",
    "\n",
    "## Get Vader Label Counts\n",
    "df_vader_counts = df_vader[['company', 'datepy', 'vader_label']]\n",
    "\n",
    "vader_counts = df_vader_counts.groupby(by=['company','datepy'])\n",
    "vader_counts = vader_counts.vader_label.value_counts()\n",
    "\n",
    "\n",
    "## Turn into DF for final integration into one big DF\n",
    "vader_dict = []\n",
    "\n",
    "for k,v in vader_counts.iteritems():\n",
    "    \n",
    "    s = {}\n",
    "    s['Name'] = k[0]\n",
    "    s['Date'] = k[1]\n",
    "    \n",
    "    if k[2] == 'Positive':\n",
    "        s['Vader_Pos_Count'] = v\n",
    "    elif k[2] == 'Neutral':\n",
    "        s['Vader_Neutral_Count'] = v\n",
    "    else:\n",
    "        s['Vader_Neg_Count'] = v\n",
    "\n",
    "    vader_dict.append(s)\n",
    "    \n",
    "vader_counts = pd.DataFrame(vader_dict)\n",
    "vader_counts = vader_counts.groupby(['Name','Date']).sum()\n",
    "vader_counts = vader_counts.fillna(0)\n",
    "vader_counts = vader_counts.reset_index(level=['Date','Name'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Merge all Sentiment Datasets</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Merge Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_senti_combined = pd.merge(svm_counts, mnb_counts, on=['Date','Name'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_senti_combined = pd.merge(df_senti_combined,logit_counts, on=['Date','Name'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_senti_combined = pd.merge(df_senti_combined,twc_counts, on=['Date','Name'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_senti_combined = pd.merge(df_senti_combined,vader_counts, on=['Date','Name'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Merge Scores, Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_senti_combined = pd.merge(df_senti_combined,df_tweetwordcount_ratio, on=['Date','Name'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_senti_combined = pd.merge(df_senti_combined,df_vader_score, on=['Date','Name'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf_price_test = pd.DataFrame()\\n\\nfor i in df_prices.Name.unique():\\n    partial_df = df_prices[df_prices.Name == i]\\n    partial_df['volatility'] = pd.rolling_std(partial_df['return'],window=2)\\n    df_price_test = df_price_test.append(partial_df,ignore_index=True)\\n    \""
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Rolling Averages and Sums over 2 days - Not Implemented in Final Dataset\n",
    "'''\n",
    "df_test = pd.DataFrame()\n",
    "\n",
    "for i in df_senti_combined.Name.unique():\n",
    "    partial_df = df_senti_combined[df_senti_combined.Name == i].rolling(2).sum()\n",
    "    df_test = df_test.append(partial_df,ignore_index=True)\n",
    "    \n",
    "df_senti_combined = df_test\n",
    "df_senti_combined.dropna()\n",
    "\n",
    "'''\n",
    "\n",
    "# For volatility over N days - Not Implemented in Final Dataset\n",
    "'''\n",
    "df_price_test = pd.DataFrame()\n",
    "\n",
    "for i in df_prices.Name.unique():\n",
    "    partial_df = df_prices[df_prices.Name == i]\n",
    "    partial_df['volatility'] = pd.rolling_std(partial_df['return'],window=2)\n",
    "    df_price_test = df_price_test.append(partial_df,ignore_index=True)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Shift date by +1 to merge with price data of next day\n",
    "df_senti_combined['Date'] = df_senti_combined.Date.map(lambda x: x + timedelta(days=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Combine Sentiment and Price </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_combined = pd.merge(df_prices,df_senti_combined, on=['Date','Name'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(807, 36)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = df_combined.dropna(how='any')\n",
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_combined['Total Tweets'] = df_combined['SVM_Neg_Count'] + df_combined['SVM_Pos_Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_combined = df_combined[['Volume','SVM_Neg_Count', 'SVM_Pos_Count', 'MNB_Neg_Count', 'MNB_Pos_Count',\n",
    "       'Logit_Neg_Count', 'Logit_Pos_Count', 'TWC_Neg_Count',\n",
    "       'TWC_Neutral_Count', 'TWC_Pos_Count', 'Vader_Neg_Count',\n",
    "       'Vader_Neutral_Count', 'Vader_Pos_Count', 'WordCountRatio',\n",
    "       'Vader_Compound', 'Vader_Neg', 'Vader_Neu', 'Vader_Pos','prev_day','avg_ret','Total Tweets',\n",
    "       'direction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nv_features = df_combined.ix[:,1:19].columns\\nimport matplotlib.pyplot as plt\\nimport matplotlib.gridspec as gridspec\\nimport seaborn as sns\\n\\nplt.figure(figsize=(12,18*4))\\ngs = gridspec.GridSpec(18, 1)\\nfor i, cn in enumerate(df_combined[v_features]):\\n    ax = plt.subplot(gs[i])\\n    sns.distplot(df_combined[cn][df_combined.class_num_label == 1], bins=50)\\n    sns.distplot(df_combined[cn][df_combined.class_num_label == 0], bins=50)\\n    ax.set_xlabel('')\\n    ax.set_title('histogram of feature: ' + str(cn))\\nplt.show()\\n\""
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See Histogram of Each Variable\n",
    "'''\n",
    "v_features = df_combined.ix[:,1:19].columns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12,18*4))\n",
    "gs = gridspec.GridSpec(18, 1)\n",
    "for i, cn in enumerate(df_combined[v_features]):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    sns.distplot(df_combined[cn][df_combined.class_num_label == 1], bins=50)\n",
    "    sns.distplot(df_combined[cn][df_combined.class_num_label == 0], bins=50)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_title('histogram of feature: ' + str(cn))\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saifi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Saifi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Saifi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Get Previous Day Direction as well \n",
    "\n",
    "df_combined_prevdaydirection = pd.DataFrame()\n",
    "\n",
    "for i in df_prices.Name.unique():\n",
    "    partial_df = df_prices[df_prices.Name == i]\n",
    "    partial_df['prev_day'] = partial_df['direction']\n",
    "    partial_df['prev_day'] = partial_df['direction'].shift(periods=1)\n",
    "    partial_df['avg_ret'] = partial_df['return'].rolling(3).mean()\n",
    "    df_combined_prevdaydirection = df_combined_prevdaydirection.append(partial_df,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Different CSV for Each Company - NOT INCLUDED IN FINAL \n",
    "'''\n",
    "count = 0\n",
    "\n",
    "for i in df_combined.Name.unique():\n",
    "    \n",
    "    df = df_combined[df_combined['Name'] == i]\n",
    "    \n",
    "    total_tweets = df['Total Tweets'].sum()\n",
    "    \n",
    "    df = df[['SVM_Neg_Count', 'SVM_Pos_Count', 'WordCountRatio',\n",
    "       'Vader_Compound', 'Vader_Neg', 'Vader_Neu', 'Vader_Pos','direction']]\n",
    "    \n",
    "    count = count + 1\n",
    "    filename = 'By_Company/' + str(int(total_tweets)) + '_' + str(count) + str(i) + '.csv'\n",
    "    \n",
    "    df.to_csv(filename,index=False)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf_combined_ONLYSNAP = df_combined[df_combined['Name'] == 'SNAP']\\ndf_combined_ONLYSNAP = df_combined_ONLYSNAP.drop('Total Tweets',axis=1)\\ndf_combined_ONLYSNAP.to_csv('Final_3rdClassifier_OnlySnap.csv', float_format='%.5f',index=False)\\n\\ndf_combined_NOSNAP = df_combined[df_combined['Name'] != 'SNAP']\\ndf_combined_NOSNAP = df_combined_NOSNAP.drop('Total Tweets',axis=1)\\ndf_combined_NOSNAP.to_csv('Final_3rdClassifier_OnlySnap.csv', float_format='%.5f',index=False)\\n\\n\""
      ]
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Write to CSV for testing in WEKA\n",
    "filename = 'Final_2ndC_PrevDayRet_AvgRet'\n",
    "\n",
    "df_combined.to_csv(filename + '_ALL.csv', float_format='%.5f',index=False)\n",
    "\n",
    "\n",
    "df_combined_20 = df_combined[df_combined['Total Tweets'] > 20]\n",
    "#df_combined_20 = df_combined_20.drop('Total Tweets',axis=1)\n",
    "df_combined_20.to_csv(filename + '_MoreThan20.csv', float_format='%.5f',index=False)\n",
    "\n",
    "\n",
    "df_combined_50 = df_combined[df_combined['Total Tweets'] > 50]\n",
    "#df_combined_50 = df_combined_50.drop('Total Tweets',axis=1)\n",
    "df_combined_50.to_csv(filename + '_MoreThan50.csv', float_format='%.5f',index=False)\n",
    "'''\n",
    "df_combined_ONLYSNAP = df_combined[df_combined['Name'] == 'SNAP']\n",
    "df_combined_ONLYSNAP = df_combined_ONLYSNAP.drop('Total Tweets',axis=1)\n",
    "df_combined_ONLYSNAP.to_csv('Final_3rdClassifier_OnlySnap.csv', float_format='%.5f',index=False)\n",
    "\n",
    "df_combined_NOSNAP = df_combined[df_combined['Name'] != 'SNAP']\n",
    "df_combined_NOSNAP = df_combined_NOSNAP.drop('Total Tweets',axis=1)\n",
    "df_combined_NOSNAP.to_csv('Final_3rdClassifier_OnlySnap.csv', float_format='%.5f',index=False)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Insert into MongoDB (Needs Date converstion to Datetime)\n",
    "#df_combined.Date = df_combined.Date.map(lambda t: datetime(t.year, t.month, t.day))\n",
    "#FinalDataset.insert_many(df_combined.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WordVector = pd.read_csv('tweetwordvector.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WordVector = WordVector.rename(columns={'Price_Date' : 'Date', 'IPO_Company' : 'Name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WordVector['Date'] = WordVector['Date'].map(lambda x: pd.to_datetime(x).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_New = pd.merge(df_New,df_combined, on=['Name','Date'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_New = df_New.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_New.to_csv('LastTry.csv',float_format='%.5f',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Adj_Close_x', 'Adj_High_x', 'Adj_Low_x', 'Adj_Open_x', 'Adj_Volume_x',\n",
       "       'Close_x', 'Date', 'Dividend_x', 'High_x', 'Low_x', 'Name', 'Open_x',\n",
       "       'Split_x', 'UID_x', 'Volume_x', '_id_x', 'class_num_label_x',\n",
       "       'direction_x', 'return_x', 'prev_day', 'avg_ret', 'Stock_Direction',\n",
       "       'Stock_Volume', 'back', 'bold', 'capital', 'close', 'first', 'free',\n",
       "       'going', 'good', 'great', 'hot', 'industrial', 'innovative', 'jagged',\n",
       "       'last', 'like', 'live', 'make', 'new', 'nice', 'open', 'outperform',\n",
       "       'right', 'see', 'short', 'smart', 'top', 'Adj_Close_y', 'Adj_High_y',\n",
       "       'Adj_Low_y', 'Adj_Open_y', 'Adj_Volume_y', 'Close_y', 'Dividend_y',\n",
       "       'High_y', 'Low_y', 'Open_y', 'Split_y', 'UID_y', 'Volume_y', '_id_y',\n",
       "       'class_num_label_y', 'direction_y', 'return_y', 'SVM_Neg_Count',\n",
       "       'SVM_Pos_Count', 'MNB_Neg_Count', 'MNB_Pos_Count', 'Logit_Neg_Count',\n",
       "       'Logit_Pos_Count', 'TWC_Neg_Count', 'TWC_Neutral_Count',\n",
       "       'TWC_Pos_Count', 'Vader_Neg_Count', 'Vader_Neutral_Count',\n",
       "       'Vader_Pos_Count', 'WordCountRatio', 'Vader_Compound', 'Vader_Neg',\n",
       "       'Vader_Neu', 'Vader_Pos'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.052379\n",
       "1      0.010391\n",
       "2     -0.005196\n",
       "3      0.021613\n",
       "4     -0.010849\n",
       "5     -0.008409\n",
       "6      0.021058\n",
       "7      0.003749\n",
       "8     -0.049000\n",
       "9      0.003980\n",
       "10     0.004660\n",
       "11     0.012623\n",
       "12     0.015495\n",
       "13     0.010773\n",
       "14     0.007090\n",
       "15    -0.001507\n",
       "16     0.006311\n",
       "17     0.010920\n",
       "18    -0.002439\n",
       "19    -0.005339\n",
       "20    -0.003375\n",
       "21     0.009053\n",
       "22    -0.005394\n",
       "23    -0.032840\n",
       "24     0.003202\n",
       "25     0.003202\n",
       "26     0.003928\n",
       "27    -0.001986\n",
       "28    -0.014908\n",
       "29     0.001323\n",
       "         ...   \n",
       "930         NaN\n",
       "931         NaN\n",
       "932         NaN\n",
       "933         NaN\n",
       "934         NaN\n",
       "935         NaN\n",
       "936         NaN\n",
       "937         NaN\n",
       "938         NaN\n",
       "939         NaN\n",
       "940         NaN\n",
       "941         NaN\n",
       "942         NaN\n",
       "943         NaN\n",
       "944         NaN\n",
       "945         NaN\n",
       "946         NaN\n",
       "947         NaN\n",
       "948         NaN\n",
       "949         NaN\n",
       "950         NaN\n",
       "951         NaN\n",
       "952         NaN\n",
       "953         NaN\n",
       "954         NaN\n",
       "955         NaN\n",
       "956         NaN\n",
       "957         NaN\n",
       "958         NaN\n",
       "959         NaN\n",
       "Name: avg_ret, dtype: float64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_New['avg_ret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
